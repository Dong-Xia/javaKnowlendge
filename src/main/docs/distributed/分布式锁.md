# 分布式锁
<!-- MarkdownTOC -->
- [概念](#概念)
- [分布式锁实现的方式](#分布式锁实现的方式)
- [基于Zookeeper实现分布式锁](#基于Zookeeper实现分布式锁)
- [Zookeeper实现分布式锁原理](#Zookeeper实现分布式锁原理)
- [Zookeeper和Redis分布式锁的比较](#Zookeeper和Redis分布式锁的比较)
<!-- /MarkdownTOC -->
## 概念
   　　在多线程并发的情况下，在一个JVM进程中，常使用锁（如：synchronized、Reentrantlock）来保证一个代码块在同一时间
   只有一个线程访问。但是在分布式的集群环境中，不同的JVM进程之间怎么保证不同节点访问同步代码块始终是线程同步呢？
   
   　　 针对分布式环境下的资源共享，通常采用分布式锁实现线程同步，实现的原理：在所有服务都共享的一个服务上去获取一个
   唯一标示，各个线程通过对该标示进行获取和释放达到同步的效果。
## 分布式锁实现的方式
   1. 基于缓存实现的分布式锁
    
   - Memcached分布式锁：利用Memcached的add命令。此命令是原子性操作，只有在key不存在的情况下，才能add成功，也就意味着线程得到了锁。
   - Redis分布式锁：和Memcached的方式类似，利用Redis的setnx命令。此命令同样是原子性操作，只有在key不存在的情况下，才能set成功。（setnx命令并不完善，后续会介绍替代方案）
    
   2. 基于Zookeeper的分布式锁
    
   利用Zookeeper的顺序临时节点，来实现分布式锁和等待队列。Zookeeper设计的初衷，就是为了实现分布式锁服务的。
    
   3. 基于数据库实现的分布式锁
   - 基于数据库表：建立一张数据锁表，通过操作表中数据来实现，需要锁住一个方法或资源，则在数据表中增加一条记录，释放锁就删除该条记录。
        
            这种实现方式存在的问题：
           (1)、这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。
           
           (2)、这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。
           
           (3)、这把锁只能是非阻塞的，因为数据的insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。
           
           (4)、这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。
   - 基于数据库排它锁（for update）
   
## 基于Zookeeper实现分布式锁
### 实现的基础：临时顺序节点
　　Zookeeper节点的概念，类似一棵树，这棵树由节点组成，这种节点叫做Znode。如图所示。
![zookeeper数据存储结构](/src/main/images/zookeeper/zookeeper数据存储结构.png)

　　Znode节点分为4中类型：
  1. 持久节点：默认的节点类型。创建节点的客户端与zookeeper断开连接后，该节点依旧存在。
  2. 持久顺序节点：所谓顺序节点，就是在创建节点时，Zookeeper根据创建的时间顺序给该节点名称进行编号。
![持久顺序节点](/src/main/images/zookeeper/持久顺序节点.jpg)
  3. 临时节点：和持久节点相反，当创建节点的客户端与zookeeper断开连接后，临时节点会被删除。
  4. 临时顺序节点：顾名思义，临时顺序节点结合和临时节点和顺序节点的特点：在创建节点时，Zookeeper根据创建的时间顺序给该节点名称进行编号；当创建节点的客户端与zookeeper断开连接后，临时节点会被删除。
### Zookeeper实现分布式锁原理
Zookeeper分布式锁恰恰应用了临时顺序节点。具体如何实现呢？让我们来看一看详细步骤：

#### 获取锁

首先，在Zookeeper当中创建一个持久节点ParentLock。当第一个客户端想要获得锁时，需要在ParentLock这个节点下面创建一个临时顺序节点 Lock1。
![Znode1](/src/main/images/zookeeper/Znode1.png)

之后，Client1查找ParentLock下面所有的临时顺序节点并排序，判断自己所创建的节点Lock1是不是顺序最靠前的一个。如果是第一个节点，则成功获得锁。
![Znode2](/src/main/images/zookeeper/Znode2.png)

这时候，如果再有一个客户端 Client2 前来获取锁，则在ParentLock下载再创建一个临时顺序节点Lock2。
![Znode3](/src/main/images/zookeeper/Znode3.png)

Client2查找ParentLock下面所有的临时顺序节点并排序，判断自己所创建的节点Lock2是不是顺序最靠前的一个，结果发现节点Lock2并不是最小的。

于是，Client2向排序仅比它靠前的节点Lock1注册Watcher，用于监听Lock1节点是否存在。这意味着Client2抢锁失败，进入了等待状态。
![Znod4](/src/main/images/zookeeper/Znode4.png)

这时候，如果又有一个客户端Client3前来获取锁，则在ParentLock下载再创建一个临时顺序节点Lock3。
![Znod5](/src/main/images/zookeeper/Znode5.png)

Client3查找ParentLock下面所有的临时顺序节点并排序，判断自己所创建的节点Lock3是不是顺序最靠前的一个，结果同样发现节点Lock3并不是最小的。

于是，Client3向排序仅比它靠前的节点Lock2注册Watcher，用于监听Lock2节点是否存在。这意味着Client3同样抢锁失败，进入了等待状态。
![Znod6](/src/main/images/zookeeper/Znode6.png)

**这样一来，Client1得到了锁，Client2监听了Lock1，Client3监听了Lock2。这恰恰形成了一个等待队列，很像是Java当中ReentrantLock所依赖的AQS（AbstractQueuedSynchronizer）**。

#### 释放锁
释放锁分为两种情况：

- 任务完成，客户端显示释放

当任务完成时，Client1会显示调用删除节点Lock1的指令
![Znod7](/src/main/images/zookeeper/Znode7.png)

- 任务执行过程中，客户端崩溃

获得锁的Client1在任务执行过程中，如果Duang的一声崩溃，则会断开与Zookeeper服务端的链接。根据临时节点的特性，相关联的节点Lock1会随之自动删除。
![Znod8](/src/main/images/zookeeper/Znode8.png)
       
由于Client2一直监听着Lock1的存在状态，当Lock1节点被删除，Client2会立刻收到通知。这时候Client2会再次查询ParentLock下面的所有节点，确认自己创建的节点Lock2是不是目前最小的节点。如果是最小，则Client2顺理成章获得了锁。
![Znod9](/src/main/images/zookeeper/Znode9.png)

同理，如果Client2也因为任务完成或者节点崩溃而删除了节点Lock2，那么Client3就会接到通知。 
![Znod10](/src/main/images/zookeeper/Znode10.png)

最终，Client3成功得到了锁。
![Znod11](/src/main/images/zookeeper/Znode11.png)
  
### Zookeeper和Redis分布式锁的比较
![zookeeper和Redis实现分布式锁比较](/src/main/images/zookeeper/zookeeper和Redis实现分布式锁比较.jpg)

## 基于缓存实现分布式锁
Redis分布式锁的基本流程并不难理解，但要想写得尽善尽美，也并不是那么容易。在这里，我们需要先了解分布式锁实现的三个核心要素：

1. 加锁

最简单的方法是使用setnx和set命令。key是锁的唯一标识，按业务来决定命名。比如想要给一种商品的秒杀活动加锁，可以给key命名为 “lock_sale_商品ID” 。而value设置成什么呢？我们可以姑且设置成1。加锁的伪代码如下：    

    setnx（key，1） 和 支持超时时间设定 set（key，1，30，NX）

当一个线程执行setnx返回1，说明key原本不存在，该线程成功得到了锁；当一个线程执行setnx返回0，说明key已经存在，该线程抢锁失败。


2. 解锁

有加锁就得有解锁。当得到锁的线程执行完任务，需要释放锁，以便其他线程可以进入。释放锁的最简单方式是执行del指令，伪代码如下：

    del（key）

释放锁之后，其他线程就可以继续执行setnx命令来获得锁。


3. 锁超时

锁超时是什么意思呢？如果一个得到锁的线程在执行任务的过程中挂掉，来不及显式地释放锁，这块资源将会永远被锁住，别的线程再也别想进来。

所以，setnx的key必须设置一个超时时间，以保证即使没有被显式释放，这把锁也要在一定时间后自动释放。setnx不支持超时参数，所以需要额外的指令，伪代码如下：

    expire（key， 30）

综合起来，我们分布式锁实现的第一版伪代码如下：

    if（setnx（key，1） == 1）{
        expire（key，30）    
        try {
            do something ......
        } finally {
            del（key）
        }
    }
上面的伪代码中，存在着三个致命问题：

1. setnx和expire的非原子性操作 
   - 设想一个极端场景，当某线程执行setnx，成功得到了锁：
   - setnx刚执行成功，还未来得及执行expire指令，节点1 Duang的一声挂掉了。
   - 这样一来，这把锁就没有设置过期时间，变得“长生不老”，别的线程再也无法获得锁了。
   
   **怎么解决呢**？setnx指令本身是不支持传入超时时间的，幸好Redis 2.6.12以上版本为set指令增加了可选参数，伪代码如下：
       
       set（key，1，30，NX） // key为唯一标示，1为value值，30是超时时间
   
   这样就可以取代setnx指令。
2. del 导致误删
   - 又是一个极端场景，假如某线程成功得到了锁，并且设置的超时时间是30秒。
   - 如果某些原因导致线程A执行的很慢很慢，过了30秒都没执行完，这时候锁过期自动释放，线程B得到了锁。
   - 随后，线程A执行完了任务，线程A接着执行del指令来释放锁。但这时候线程B还没执行完，线程A实际上删除的是线程B加的锁。
  
  **怎么避免这种情况呢**？可以在del释放锁之前做一个判断，验证当前的锁是不是自己加的锁。
  
  至于具体的实现，**可以在加锁的时候把当前的线程ID当做value，并在删除之前验证key对应的value是不是自己线程的ID**。
  
      加锁：
      String threadId = Thread.currentThread().getId()
      set（key，threadId ，30，NX） 
      
      解锁：
      if（threadId .equals(redisClient.get(key))）{
          del(key)
      }
      
  但是，这样做又隐含了一个新的问题，判断和释放锁是两个独立操作，不是原子性。
  
  我们都是追求极致的程序员，所以这一块要用Lua脚本来实现：
  
  String luaScript = "if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end";
  
  redisClient.eval(luaScript , Collections.singletonList(key), Collections.singletonList(threadId));
  
  这样一来，验证和删除过程就是原子操作了。


   